# ğŸš€ GraphRAG v2.0 - GuÃ­a RÃ¡pida

## ğŸ“‹ Contenido

- [InstalaciÃ³n](#instalaciÃ³n)
- [Uso BÃ¡sico](#uso-bÃ¡sico)
- [EvaluaciÃ³n v1.5 vs v2.0](#evaluaciÃ³n)
- [OptimizaciÃ³n para Raspberry Pi](#optimizaciÃ³n-raspberry-pi)
- [PrÃ³ximos Pasos](#prÃ³ximos-pasos)

---

## ğŸ”§ InstalaciÃ³n

### Requisitos Previos
- Python 3.8+
- 2GB RAM mÃ­nimo (recomendado 4GB+)
- 500MB espacio en disco

### InstalaciÃ³n en Raspberry Pi 5

```bash
# 1. Actualizar sistema
sudo apt update && sudo apt upgrade -y

# 2. Instalar dependencias del sistema
sudo apt install -y python3-pip python3-dev build-essential

# 3. Crear entorno virtual (recomendado)
python3 -m venv venv_graphrag
source venv_graphrag/bin/activate

# 4. Actualizar pip
pip install --upgrade pip

# 5. Instalar dependencias de GraphRAG v2.0
pip install -r requirements_v2.txt

# Nota: La primera instalaciÃ³n descargarÃ¡ ~200MB de modelos
```

### InstalaciÃ³n RÃ¡pida (Cualquier Sistema)

```bash
pip install rdflib sentence-transformers scikit-learn numpy
```

---

## ğŸ® Uso BÃ¡sico

### Modo Interactivo

```bash
python graphrag_v2.py
```

Esto iniciarÃ¡:
1. Carga del grafo TTL
2. Carga del modelo de embeddings (~80MB)
3. PrecÃ¡lculo de embeddings de todas las entidades
4. Modo interactivo de preguntas

### Uso ProgramÃ¡tico

```python
from graphrag_v2 import GraphRAG_v2

# Inicializar
rag = GraphRAG_v2("qoyllurity.ttl")

# Hacer preguntas
respuesta = rag.responder(
    "Â¿QuÃ© hacen los ukukus?",
    modo="hibrido",  # 'semantico', 'lexico', o 'hibrido'
    verbose=True     # Muestra debug info
)

print(respuesta)
```

### Modos de BÃºsqueda

**1. SemÃ¡ntico** (Recomendado para preguntas naturales)
```python
# Entiende sinÃ³nimos y parÃ¡frasis
respuesta = rag.responder("Â¿CuÃ¡l es la funciÃ³n de los ukumaris?", modo="semantico")
```

**2. LÃ©xico** (Compatible con v1.5)
```python
# BÃºsqueda por palabras clave exactas
respuesta = rag.responder("ukukus danza", modo="lexico")
```

**3. HÃ­brido** (Mejor balance) â­
```python
# Combina ambos con pesos ajustables
respuesta = rag.responder("Â¿DÃ³nde estÃ¡ el santuario?", modo="hibrido")
```

### Guardar y Cargar CachÃ©

```python
# Guardar embeddings para carga rÃ¡pida
rag.guardar_cache("cache_embeddings.pkl")

# En siguiente ejecuciÃ³n, cargar desde cachÃ©
rag2 = GraphRAG_v2("qoyllurity.ttl")
if rag2.cargar_cache("cache_embeddings.pkl"):
    print("âœ… CachÃ© cargado - inicio rÃ¡pido!")
```

---

## ğŸ“Š EvaluaciÃ³n

### Ejecutar Suite de EvaluaciÃ³n Completa

```bash
python evaluar_v15_vs_v20.py
```

Esto ejecutarÃ¡:
- â±ï¸ **Test de latencia** (v1.5 vs v2.0 en 3 modos)
- ğŸ¯ **Test de calidad** (precisiÃ³n de respuestas)
- ğŸ”¤ **Test de sinÃ³nimos** (capacidad semÃ¡ntica de v2.0)

### Resultados Esperados

| MÃ©trica | v1.5 | v2.0 (hÃ­brido) |
|---------|------|----------------|
| Latencia media | ~80ms | ~250ms |
| RAM uso | ~100MB | ~500MB |
| PrecisiÃ³n | 75-80% | 85-90% |
| SinÃ³nimos | âŒ No | âœ… SÃ­ |

### Benchmark Manual

```python
from graphrag_v2 import GraphRAG_v2, benchmark

rag = GraphRAG_v2("qoyllurity.ttl")

queries = [
    "Â¿QuÃ© es Qoyllur Rit'i?",
    "Â¿DÃ³nde estÃ¡ el santuario?",
    "Â¿QuÃ© hacen los ukukus?",
]

benchmark(rag, queries)
```

---

## ğŸ”§ OptimizaciÃ³n para Raspberry Pi

### 1. Reducir Uso de RAM

```python
# Usar modelo mÃ¡s pequeÃ±o (inglÃ©s only, pero mÃ¡s rÃ¡pido)
rag = GraphRAG_v2(
    "qoyllurity.ttl",
    model_name="all-MiniLM-L6-v2"  # 80MB vs 120MB
)
```

### 2. Ajustar Batch Size

```python
# En _compute_embeddings(), cambiar:
self.embeddings = self.model.encode(
    self.entity_texts,
    batch_size=16,  # Reducir de 32 a 16 para menos RAM
    show_progress_bar=True
)
```

### 3. Usar Swap si es Necesario

```bash
# Agregar 2GB de swap en RPi
sudo fallocate -l 2G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Hacer permanente
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

### 4. Compilar PyTorch con Optimizaciones ARM

```bash
# Para mÃ¡ximo rendimiento (opcional, tarda ~2 horas)
pip install torch --no-cache-dir --index-url https://download.pytorch.org/whl/cpu
```

---

## ğŸ¯ ComparaciÃ³n RÃ¡pida: Â¿CuÃ¡ndo usar quÃ©?

### Usar v1.5 si:
- âœ… Necesitas latencia <100ms
- âœ… RAM muy limitada (<512MB disponible)
- âœ… Consultas con tÃ©rminos exactos
- âœ… Sistema embebido simple

### Usar v2.0 si:
- âœ… Preguntas en lenguaje natural
- âœ… Usuarios escriben con sinÃ³nimos/parÃ¡frasis
- âœ… 200-300ms de latencia es aceptable
- âœ… Tienes >1GB RAM disponible
- âœ… Necesitas mejor calidad de respuestas

### Modo Recomendado v2.0
```python
# Usar modo hÃ­brido con alpha=0.7
respuesta = rag.responder(query, modo="hibrido")
# 70% peso semÃ¡ntico + 30% lÃ©xico = mejor balance
```

---

## ğŸ“ˆ PrÃ³ximos Pasos

### Cuando v2.0 funcione bien y necesites MÃS:

**â†’ v4.0 - LLM Small + RAG Completo**
- Genera respuestas en lenguaje natural
- Query decomposition
- Razonamiento bÃ¡sico multi-hop
- Modelo: Phi-3-mini-4k (2.3GB)
- RAM: ~3GB
- Latencia: ~2s

### MigraciÃ³n a v4.0

1. Asegurar que v2.0 funciona bien
2. Medir que la calidad justifica esperar 2s
3. Instalar llama-cpp-python
4. Descargar modelo Phi-3-mini-4k
5. Integrar generaciÃ³n de lenguaje natural

---

## ğŸ› Troubleshooting

### Error: "No module named 'sentence_transformers'"
```bash
pip install sentence-transformers
```

### Error: "Killed" durante compute_embeddings
- **Causa**: RAM insuficiente
- **SoluciÃ³n**: Agregar swap o reducir batch_size

### Latencia muy alta (>1s)
- **Causa**: Modelo muy grande o CPU lenta
- **SoluciÃ³n**: Usar modelo mÃ¡s pequeÃ±o: `all-MiniLM-L6-v2`

### Embeddings no se guardan en cachÃ©
```python
# Verificar permisos
import os
print(os.access('.', os.W_OK))  # Debe ser True
```

---

## ğŸ“š Referencias

- Modelo embeddings: [sentence-transformers](https://www.sbert.net/)
- Modelos multilingÃ¼es: [paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)
- Roadmap completo: Ver `ROADMAP_GraphRAG_RPi5.md`

---

## ğŸ’¡ Tips y Trucos

### 1. Primera ejecuciÃ³n lenta
Es normal. El modelo se descarga una vez (~120MB).

### 2. Ajustar peso hÃ­brido
```python
# MÃ¡s peso a semÃ¡ntico (mejor para lenguaje natural)
resultados = rag.buscar_hibrido(query, alpha=0.8)

# MÃ¡s peso a lÃ©xico (mejor para tÃ©rminos tÃ©cnicos)
resultados = rag.buscar_hibrido(query, alpha=0.5)
```

### 3. Ver top-K resultados
```python
# Ver las 10 entidades mÃ¡s relevantes
results = rag.buscar_semantico("ukukus", top_k=10)
for ent_id, score in results:
    ent = rag.entidades[ent_id]
    print(f"{ent['labels'][0]}: {score:.3f}")
```

### 4. BÃºsqueda solo sin respuesta
```python
# Solo recuperar entidades, sin generar respuesta
results = rag.buscar_hibrido("lomada", top_k=5)
```

---

## ğŸ“ Arquitectura v2.0

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding Generation       â”‚
â”‚  (SentenceTransformer)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Semanticâ”‚    â”‚ Lexical â”‚   â”‚ Hybrid   â”‚
    â”‚ Search â”‚    â”‚ Search  â”‚   â”‚ (Î±=0.7)  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Top-K Entities  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Intent Detection â”‚
            â”‚  (Rule-based)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Template         â”‚
            â”‚ Selection        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Final Response  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Â¿Preguntas? Â¿Problemas? Â¡Pregunta! ğŸš€**
